{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Counting Word Frequencies with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The debates that occur in the Canadian Parliament are transcribed and published in a document known as <i>Hansard</i>. Since 2006, <i>Hansard</i> has been available for public download in <code>.xml</code> file format. To date, this archive contains over 57 million words. By converting these transcripts from <code>.xml</code> to <code>.txt</code>, the files can be processed in numerous ways with the <i>Python</i> coding language.</p>\n",
    "<p>The purpose of this document is to illustrate the counting of specific words in the collection of text files that will be referred to from now on as the <b><span style=\"cursor:help;\" title=\"a large and structured set of texts\">corpus</b></span>. Terms that may be unfamiliar to the reader are highlighted in <b><span style=\"cursor:help;\" title=\"hover the mouse over the word to read the definition\">bold</span></b>, and contain a definition that can be accessed by hovering the mouse pointer over the word. While it is not necessary to read every line of code to understand the process, explanatory sections within the code are marked with a # and coloured light blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part 1: Determining the word frequency for a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin working with a piece of text, it must be loaded into and read by <i>Python</i>. Rather than altering (and potentially irreversibly changing) our original text file, we will work with the contents of the file as a <b><span style=\"cursor:help;\" title=\"any finite sequence of characters (i.e., letters, numerals, symbols and punctuation marks)\">string</span></b> of text contained within a <b><span style=\"cursor:help;\" title=\"a value that can change, depending on conditions or on information passed to the program\">variable</span></b>. Now we can close our original text file, keeping it intact.\n",
    "\n",
    "In the next piece of code we will open the file that contains the textual transcripts of all of the Parliamentary debates that occured in the House of Commons during the year 2006.\n",
    "\n",
    "While working with one long string may be useful for other applications, our purposes require that we split the text into pieces. In this case, each word will become it's own unique string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. open the text file\n",
    "infile = open('data1/2006.txt')\n",
    "# 2. read the file and assign it to the variable 'text'\n",
    "text = infile.read()\n",
    "# 3. close the text file\n",
    "infile.close()\n",
    "# 4. split the variable 'text' into distinct word strings\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded our file, we can begin to work on it. <i>Python</i> offers us a lot of pre-built tools to make the task of coding easier. Some of the most commonly used tools are known as <b><span style=\"cursor:help;\" title=\"a set of instructions that performs a specific task\">functions</span></b>. Functions are useful for automating tasks that would otherwise require a repetitive amount of coding. While <i>Python</i> has many built-in functions, the language's true power comes from the ability to define unique functions based on programming needs. In the code above, we've already used four <i>Python</i> functions: <code>open</code>, <code>read</code>, <code>close</code>, and <code>split</code>.</p>\n",
    "<p>In the next piece of code we will define our own funtion, called <code>count_in_list</code>. This function will allow us to count the occurence of any word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. define the'count_in_list' function\n",
    "def count_in_list(item_to_count, list_to_search):\n",
    "    \"Counts the number of a specified word within a list of words\"\n",
    "    number_of_hits = 0\n",
    "    for item in list_to_search:\n",
    "        if item == item_to_count:\n",
    "            number_of_hits += 1\n",
    "    return number_of_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the function for any word we choose. The next example shows that there are <u>195</u> occurences of the word <code>privacy</code> contained in the transcripts for the year 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'privacy': 195\n"
     ]
    }
   ],
   "source": [
    "# 6. here the function counts the instances of the word 'privacy'\n",
    "print \"Instances of the word \\'privacy\\':\", (count_in_list(\"privacy\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are two distinct problems here, centred around the fact that our function is only counting the string <code>privacy</code> exactly as it appears.</p>\n",
    "<p>The first problem is that text strings are case-sensitive. If the word contains UPPERCASE and lowercase letters, the word that is searched for will only be counted if the cases match exactly. The following example counts the number of instances of <code>Privacy</code> with the first letter capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'Privacy': 161\n"
     ]
    }
   ],
   "source": [
    "print \"Instances of the word \\'Privacy\\':\", (count_in_list(\"Privacy\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more extreme example to illustrate the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'pRiVaCy': 0\n"
     ]
    }
   ],
   "source": [
    "print \"Instances of the word \\'pRiVaCy\\':\",(count_in_list(\"pRiVaCy\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second problem is that of punctuation. Much like words are case-sensitive, they are also punctuation-sensitive. If a piece of punctuation has been included in the string, it will be included in the search. Here we count the occurrences of <code>privacy,</code> shown here with a comma after the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'privacy,': 19\n"
     ]
    }
   ],
   "source": [
    "print \"Instances of the word \\'privacy,\\':\", (count_in_list(\"privacy,\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we count <code>privacy.</code>, with the word followed by a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'privacy.': 32\n"
     ]
    }
   ],
   "source": [
    "print \"Instances of the word \\'privacy.\\':\",(count_in_list(\"privacy.\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could comb through the text to find all of the different instantiations of <code>privacy</code>, and then run the code for each one and add together all of the numbers, but that would be time consuming and potentially inaccurate. Instead, we must process the text further to make the text uniform. In this case we want to make all of the characters lowercase, and remove all of the punctuation.</p>\n",
    "<p>To achieve this we will build another custom built function, defined below as <code>clean_text</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"Renders all text lowercase and removes punctuation\"\n",
    "    lower_text = text.lower()\n",
    "    punctuation = '!@#$%^&*()_-+={}[]:;\"\\'|<>,.?/~`'\n",
    "    clean_text = \"\"\n",
    "    for character in lower_text:\n",
    "        if character not in punctuation:\n",
    "            clean_text += character\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reload the text file, and run the <code>clean_text</code> function before we split the text into distinct words, or <b><span style=\"cursor:help;\" title=\"an individual occurence of a symbol or string\">tokens</span></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open('data1/2006.txt')\n",
    "text = infile.read()\n",
    "infile.close()\n",
    "\n",
    "#here we call the custom text cleaning function\n",
    "clean = clean_text(text)\n",
    "\n",
    "words = clean.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we count the instances of <code>privacy</code>, we are presented with a total of <u>409</u> instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'privacy': 409\n"
     ]
    }
   ],
   "source": [
    "print \"Instances of the word \\'privacy\\':\", (count_in_list(\"privacy\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part 2: Determining Word Frequencies for the Entire Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how this compares to the rest of the corpus. To accomplish this, we must write another function that will read all of the text files in our file folder.\n",
    "\n",
    "<p>First we need to introduce a function of <i>Python</i> that we've yet to see: modules. Modules are packages of functions and code that serve specific purposes. These are much like functions, but more complex.\n",
    "\n",
    "<p>The next piece of code imports a module called <code>os</code>, specifically the function <code>listdir</code>. We will use <code>listdir</code> to print a list of all the files in a specific directory. Each of the listed files corresponds to a textual transcript of Hansard. Each file represents a complete transcript for every published debate between 2006 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '2006.txt',\n",
       " '2007.txt',\n",
       " '2008.txt',\n",
       " '2009.txt',\n",
       " '2010.txt',\n",
       " '2011.txt',\n",
       " '2012.txt',\n",
       " '2013.txt',\n",
       " '2014.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports the os module\n",
    "from os import listdir\n",
    "# calls the listdir function to list the files in a specific directory\n",
    "listdir(\"data1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can display the contents of a directory by using the <code>listdir</code> function, <i>Python</i> needs those names stored in a list in order to iterate over it. We also want to specify that only files with the extension <code>.txt</code> are included. Here we create another function called <code>list_textfiles</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_textfiles(directory):\n",
    "    \"Return a list of filenames ending in '.txt'\"\n",
    "    textfiles = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            textfiles.append(directory + \"/\" + filename)\n",
    "    return textfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than writing code to open each file individually, we can create another custom function to open the file we pass to it. We'll call this one read_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"Read the contents of FILENAME and return as a string.\"\n",
    "    infile = open(filename)\n",
    "    contents = infile.read()\n",
    "    infile.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open all of the files in our directory, strip each file of uppercase letters and punctuation, split the whole of each text into tokens, and store all the data as separate lists in our variable <code>corpus</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for filename in list_textfiles(\"data1\"):\n",
    "    # reads the file\n",
    "    text = read_file(filename)\n",
    "    # removes the punctuation and changes Uppercase to lower\n",
    "    clean = clean_text(text)\n",
    "    # splits the text into tokens\n",
    "    words = clean.split()\n",
    "    # creates a set of word lists for each file\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to make sure the code worked by using the <code>len</code> function to count the number of items in our <code>corpus</code> list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 files in the list, named:  data1/2006.txt, data1/2007.txt, data1/2008.txt, data1/2009.txt, data1/2010.txt, data1/2011.txt, data1/2012.txt, data1/2013.txt, data1/2014.txt\n"
     ]
    }
   ],
   "source": [
    "print\"There are\", len(corpus), \"files in the list, named: \", ', '.join(list_textfiles('data1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to make the names of the files more readable. First we'll have to strip the the file extension <code>.txt</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import splitext\n",
    "\n",
    "def remove_ext(filename):\n",
    "    \"Removes the file extension, such as .txt\"\n",
    "    name, extension = splitext(filename)\n",
    "    return name\n",
    "\n",
    "for files in list_textfiles('data1'):\n",
    "    remove_ext(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to remove the <code>data1/</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "def remove_dir(filepath):\n",
    "    \"Removes the path from the file name\"\n",
    "    name = basename(filepath)\n",
    "    return name\n",
    "for files in list_textfiles('data1'):\n",
    "    remove_dir(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll write a function to tie the two functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(filepath):\n",
    "    \"Removes the path and file extension from the file name\"\n",
    "    filename = remove_ext(filepath)\n",
    "    name = remove_dir(filename)\n",
    "    return name\n",
    "\n",
    "filenames = []\n",
    "for files in list_textfiles('data1'):\n",
    "    files = get_filename(files)\n",
    "    filenames.append(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display a readable list of the files within our directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 files in the list, named: 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014 .\n"
     ]
    }
   ],
   "source": [
    "print\"There are\", len(corpus), \"files in the list, named:\", ', '.join(filenames),\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step involves iterating through both lists: <code>corpus</code> and <code>filenames</code>, in order to generate a word frequency for each file in the corpus. For this we will use <i>Python's</i> <code>zip</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'privacy' in 2006 : 409\n",
      "Instances of the word 'privacy' in 2007 : 298\n",
      "Instances of the word 'privacy' in 2008 : 273\n",
      "Instances of the word 'privacy' in 2009 : 679\n",
      "Instances of the word 'privacy' in 2010 : 672\n",
      "Instances of the word 'privacy' in 2011 : 750\n",
      "Instances of the word 'privacy' in 2012 : 667\n",
      "Instances of the word 'privacy' in 2013 : 1100\n",
      "Instances of the word 'privacy' in 2014 : 1805\n"
     ]
    }
   ],
   "source": [
    "for words, names in zip(corpus, filenames):\n",
    "    print\"Instances of the word \\'privacy\\' in\",names, \":\", count_in_list(\"privacy\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's exciting about this code is that we can now search the entire corpus for any word we choose. Let's search for <code>information</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'information' in 2006 : 3421\n",
      "Instances of the word 'information' in 2007 : 2926\n",
      "Instances of the word 'information' in 2008 : 2262\n",
      "Instances of the word 'information' in 2009 : 4522\n",
      "Instances of the word 'information' in 2010 : 4960\n",
      "Instances of the word 'information' in 2011 : 4151\n",
      "Instances of the word 'information' in 2012 : 5135\n",
      "Instances of the word 'information' in 2013 : 4076\n",
      "Instances of the word 'information' in 2014 : 6244\n"
     ]
    }
   ],
   "source": [
    "for words, names in zip(corpus, filenames):\n",
    "    print\"Instances of the word \\'information\\' in\",names, \":\", count_in_list(\"information\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about <code>ethics</code>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances of the word 'ethics' in 2006 : 533\n",
      "Instances of the word 'ethics' in 2007 : 217\n",
      "Instances of the word 'ethics' in 2008 : 791\n",
      "Instances of the word 'ethics' in 2009 : 405\n",
      "Instances of the word 'ethics' in 2010 : 542\n",
      "Instances of the word 'ethics' in 2011 : 256\n",
      "Instances of the word 'ethics' in 2012 : 326\n",
      "Instances of the word 'ethics' in 2013 : 694\n",
      "Instances of the word 'ethics' in 2014 : 392\n"
     ]
    }
   ],
   "source": [
    "for words, names in zip(corpus, filenames):\n",
    "    print\"Instances of the word \\'ethics\\' in\",names, \":\", count_in_list(\"ethics\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While word frequencies, by themselves, do not give us a tremendous amount of contextual information, they are a valuable first step in conducting large scale text analyses. For instance, returning to our frequency list for <code>privacy</code>, we can observe a general trend suggesting that the use of <code>privacy</code> has been increasing between 2006 and now. It is important to note that our calculations are a raw number. For a more contextual analysis we could calculate how many times the Parliament was in session during each period, or perhaps we could compare the word <code>privacy</code> to the total amount of words in each file.</p>\n",
    "<p>Stay tuned for the next section: <i>Adding Context to Word Frequency Counts</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
